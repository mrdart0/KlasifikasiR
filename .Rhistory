method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf,
slice=list(factor(y_train), x_train_tf=4))
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf,
slice=list(factor(y_train), x_test_tf=4))
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf,
slice=list(factor(y_train)= 3, x_test_tf=4))
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf)
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(x_test_tf, pred)
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(factor(y_test), pred)
xtab
plot(factor(y_test),x_train_tf)
plot(factor(y_test),x_test_tf)
plot(factor(y_test),pred)
View(svm1)
svm1[["SV"]]
table(svm1$SV)
table(svm1$nu)
table(svm1$call)
table(svm1$decision.values)
table(svm1$terms)
svm1$terms
svm1$nclasses
svm1$decision.values
svm1$nSV
svm1$SV
table(svm1$SV)
plot(x_train_tf)
plot(x_train_tf,adaptasi~beri)
plot(tf)
plot(x_test_tf)
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10,scale = FALSE)
summary(svm1)
plot(factor(y_train),x_train_tf)
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10,scale = TRUE)
summary(svm1)
plot(factor(y_train),x_train_tf)
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10,scale = FALSE)
summary(svm1)
plot(factor(y_train),x_train_tf)
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10,scale = FALSE)
summary(svm1)
plot(factor(y_train),x_train_tf[,0])
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10,scale = FALSE)
summary(svm1)
plot(factor(y_train),x_train_tf[0])
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10,scale = FALSE)
summary(svm1)
plot(factor(y_train),x_train_tf[])
plot(factor(y_test),pred,type = "class")
pred <- predict(svm1,x_test_tf,type = "class")
#table(pred)
xtab <- table(factor(y_test), pred)
xtab
plot(pred)
hasil_svm <- confusionMatrix(table(factor(y_test), pred))
hasil_svm
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(factor(y_test), pred)
xtab
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf)
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(factor(y_test), pred)
xtab
plot(pred)
hasil_svm <- confusionMatrix(table(factor(y_test), pred))
hasil_svm
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
download.file(url, 'wdbc.csv')
columnNames <- c("id","diagnosis","radius_mean","texture_mean","perimeter_mean",
"area_mean","smoothness_mean","compactness_mean","concavity_mean",
"concave_points_mean","symmetry_mean","fractal_dimension_mean",
"radius_se","texture_se","perimeter_se","area_se","smoothness_se",
"compactness_se","concavity_se","concave_points_se","symmetry_se",
"fractal_dimension_se","radius_worst","texture_worst","perimeter_worst",
"area_worst","smoothness_worst","compactness_worst","concavity_worst",
"concave_points_worst","symmetry_worst","fractal_dimension_worst")
#wdbc <- read_csv(url, col_names = columnNames, col_types = NULL)
wdbc <- read.csv('wdbc.csv', header = FALSE, col.names = columnNames)
# Create diagnosis vector
wdbc$diagnosis <- as.numeric(wdbc$diagnosis == "M")
library(corrplot)
library(dplyr)
library(caret)
# Find attributes that are highly corrected (ideally >0.90)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9)
library(corrplot)
library(dplyr)
wdbc.data <- wdbc[,c(3:32)]
correlationMatrix <- cor(wdbc.data)
corrplot(correlationMatrix, order = "hclust", tl.cex = 1, addrect = 8)
ncol(wdbc.data)
nrow(wdbc.data)
# Create diagnosis vector
wdbc$diagnosis <- as.numeric(wdbc$diagnosis == "M")
columnNames <- c("id","diagnosis","radius_mean","texture_mean","perimeter_mean",
"area_mean","smoothness_mean","compactness_mean","concavity_mean",
"concave_points_mean","symmetry_mean","fractal_dimension_mean",
"radius_se","texture_se","perimeter_se","area_se","smoothness_se",
"compactness_se","concavity_se","concave_points_se","symmetry_se",
"fractal_dimension_se","radius_worst","texture_worst","perimeter_worst",
"area_worst","smoothness_worst","compactness_worst","concavity_worst",
"concave_points_worst","symmetry_worst","fractal_dimension_worst")
#wdbc <- read_csv(url, col_names = columnNames, col_types = NULL)
wdbc <- read.csv('wdbc.csv', header = FALSE, col.names = columnNames)
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
download.file(url, 'wdbc.csv')
View(wdbc)
library(corrplot)
library(dplyr)
library(caret)
# Find attributes that are highly corrected (ideally >0.90)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9)
# Remove correlated variables
wdbc.data[,highlyCorrelated]
# Convert the features of the data: wdbc.data
wdbc.data <- as.matrix(wdbc.data)
# Set the row names of wdbc.data
row.names(wdbc.data) <- wdbc$id
# Create diagnosis vector
diagnosis <- as.numeric(wdbc$diagnosis)
wdbc.pcov <- prcomp(wdbc.data, scores = TRUE, cor=TRUE, center = TRUE, scale = TRUE)
summary(wdbc.pcov)
screeplot(wdbc.pcov,type = "line")
abline(h = 2.5, col = "red", lty = 3)
wdbc.pcs <- wdbc.pcov$x[,1:2]
head(wdbc.pcs)
# Create diagnosis vector
wdbc.pcst <- wdbc.pcs
wdbc.pcst <- cbind(wdbc.pcs, diagnosis)
head(wdbc.pcst)
dataset <- wdbc.pcst
dataset
library(caTools)
set.seed(123)
split = sample.split(dataset[,3], SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
str(training_set)
library(e1071)
classifier = svm(formula = diagnosis ~ .,
data = training_set,
type = 'C-classification',
kernel = "radial", gamma = 1, cost = 1, skala = TRUE)
library(textclean)
library(devtools)
library(katadasaR)
library(tokenizers)
library(stopwords)
library(dplyr)
library(proxy)
library(tm)
library(e1071)
library(kernlab)
library(caret)
library(gensvm)
library(corrplot)
library(superml)
library(remotes)
data_latih <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\data-latih.csv')
data_latih <- data_latih$Kalimat %>%
as.character()
data_latih <- strip(data_latih)
stemming_latih <- function(x){
paste(lapply(x,katadasar),collapse = " ")
}
data_latih <- lapply(tokenize_words(data_latih[]),stemming_latih)
hasil_preprocessing_latih <- tokenize_words(data_latih)
hasil_preprocessing_latih
data_uji <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\data-uji.csv')
data_uji <- data_uji$Kalimat %>%
as.character()
data_uji <- strip(data_uji)
data_uji <- data_uji %>%
as.data.frame() %>%
distinct()
data_uji <- as.character(data_uji$.)
stemming_uji <- function(x){
paste(lapply(x,katadasar),collapse = " ")
}
data_uji <- lapply(tokenize_words(data_uji[]),stemming_uji)
hasil_preprocessing_uji <- tokenize_words(data_uji)
myStopwords <- readLines("C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\idstopwords.txt")
hasil_preprocessing_uji <- as.character(hasil_preprocessing_uji)
hasil_preprocessing_uji <- tokenize_words(hasil_preprocessing_uji, stopwords = myStopwords)
hasil_preprocessing_uji
tf <- TfIdfVectorizer$new(smooth_idf = TRUE, min_df = 0.3)
tf$fit(hasil_preprocessing_latih)
tf_uji <- tf$transform(hasil_preprocessing_uji)
tf_uji
TFIDF_latih <- function(hasil_preprocessing_latih){
#tf
tf_latih <- Corpus(VectorSource(hasil_preprocessing_latih))
tf_latih <- TermDocumentMatrix(tf_latih) %>% as.matrix()
#idf
idf_latih <- log(ncol(tf_latih) /  (1 + rowSums(tf_latih != 0))) %>% diag()
tf_idf_latih <- crossprod(tf_latih,idf_latih)
colnames(tf_idf_latih) <- rownames(tf_latih)
return(tf_idf_latih / sqrt(rowSums(tf_idf_latih^2)))
}
TFIDF_latih <- TFIDF_latih(hasil_preprocessing_latih)
TFIDF_latih
dataklasifikasi <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\dattrain.csv',header = TRUE)
dataklasifikasi = as.data.frame(dataklasifikasi)
dataklasifikasi$Pernyataan = factor(dataklasifikasi$Pernyataan)
datauji <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\testingfix.csv',header = TRUE)
datauji = as.data.frame(datauji)
datauji$Pernyataan = factor(datauji$Pernyataan)
prediction <- predict(svm1, dataklasifikasi)
library(textclean)
library(devtools)
library(katadasaR)
library(tokenizers)
library(stopwords)
library(dplyr)
library(proxy)
library(tm)
library(e1071)
library(kernlab)
library(caret)
library(gensvm)
library(corrplot)
library(superml)
library(remotes)
library(RTextTools)
data_latih <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\data-latih2.csv')
data_uji <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\data-uji.csv')
y_train <- data_latih$Pernyataan
y_test <- data_uji$Pernyataan
pre_latih <- data_latih$Kalimat %>%
as.character()
pre_latih <- strip(pre_latih)
stemming_latih <- function(x){
paste(lapply(x,katadasar),collapse = " ")
}
pre_latih <- lapply(tokenize_words(pre_latih),stemming_latih)
hasil_preprocessing_latih <- tokenize_words(pre_latih)
hasil_preprocessing_latih
pre_uji <- data_uji$Kalimat %>%
as.character()
pre_uji<- strip(pre_uji)
stemming_uji <- function(x){
paste(lapply(x,katadasar),collapse = " ")
}
pre_uji <- lapply(tokenize_words(pre_uji),stemming_uji)
hasil_preprocessing_uji <- tokenize_words(pre_uji)
hasil_preprocessing_uji
TFIDF_latih <- function(hasil_preprocessing_latih){
#tf
tf_latih <- Corpus(VectorSource(hasil_preprocessing_latih))
tf_latih <- TermDocumentMatrix(tf_latih) %>% as.matrix()
#idf
idf_latih <- log(ncol(tf_latih) /  (1 + rowSums(tf_latih != 0))) %>% diag()
tf_idf_latih <- crossprod(tf_latih,idf_latih)
colnames(tf_idf_latih) <- rownames(tf_latih)
return(tf_idf_latih / sqrt(rowSums(tf_idf_latih^2)))
}
TFIDF_latih <- TFIDF_latih(hasil_preprocessing_latih)
TFIDF_latih
tf <- TfIdfVectorizer$new()
x_train_tf <- tf$fit_transform(hasil_preprocessing_latih)
x_test_tf <- tf$transform(hasil_preprocessing_uji)
x_test_tf
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf)
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(factor(y_test), pred)
xtab
plot(pred)
hasil_svm <- confusionMatrix(table(factor(y_test), pred))
hasil_svm
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(actual = factor(y_test), pred)
xtab
pred <- predict(svm1,x_test_tf)
#table(pred)
xtab <- table(actual = factor(y_test), prediksi = pred)
xtab
pred <- predict(svm1,x_test_tf)
pred2 <- predict(svm1,x_train_tf)
#table(pred)
xtab <- table(actual = factor(y_test), prediksi = pred)
xtab
plot(pred2)
xtab2 <- table(actual = factor(y_train), prediksi = pred2)
xtab2
plot(pred2)
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
download.file(url, 'wdbc.csv')
columnNames <- c("id","diagnosis","radius_mean","texture_mean","perimeter_mean",
"area_mean","smoothness_mean","compactness_mean","concavity_mean",
"concave_points_mean","symmetry_mean","fractal_dimension_mean",
"radius_se","texture_se","perimeter_se","area_se","smoothness_se",
"compactness_se","concavity_se","concave_points_se","symmetry_se",
"fractal_dimension_se","radius_worst","texture_worst","perimeter_worst",
"area_worst","smoothness_worst","compactness_worst","concavity_worst",
"concave_points_worst","symmetry_worst","fractal_dimension_worst")
#wdbc <- read_csv(url, col_names = columnNames, col_types = NULL)
wdbc <- read.csv('wdbc.csv', header = FALSE, col.names = columnNames)
# Create diagnosis vector
wdbc$diagnosis <- as.numeric(wdbc$diagnosis == "M")
library(corrplot)
library(dplyr)
wdbc.data <- wdbc[,c(3:32)]
correlationMatrix <- cor(wdbc.data)
corrplot(correlationMatrix, order = "hclust", tl.cex = 1, addrect = 8)
ncol(wdbc.data)
nrow(wdbc.data)
library(corrplot)
library(dplyr)
library(caret)
# Find attributes that are highly corrected (ideally >0.90)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9)
# Remove correlated variables
wdbc.data[,highlyCorrelated]
wdbc.data <- wdbc.data[,-highlyCorrelated]
# Number of columns after removing correlated variables
wdbc.data
ncol(wdbc.data)
nrow(wdbc.data)
# Convert the features of the data: wdbc.data
wdbc.data <- as.matrix(wdbc.data)
# Set the row names of wdbc.data
row.names(wdbc.data) <- wdbc$id
# Create diagnosis vector
diagnosis <- as.numeric(wdbc$diagnosis)
wdbc.pcov <- prcomp(wdbc.data, scores = TRUE, cor=TRUE, center = TRUE, scale = TRUE)
summary(wdbc.pcov)
screeplot(wdbc.pcov,type = "line")
abline(h = 2.5, col = "red", lty = 3)
wdbc.pcs <- wdbc.pcov$x[,1:2]
head(wdbc.pcs)
# Create diagnosis vector
wdbc.pcst <- wdbc.pcs
wdbc.pcst <- cbind(wdbc.pcs, diagnosis)
head(wdbc.pcst)
dataset <- wdbc.pcst
dataset
library(caTools)
set.seed(123)
split = sample.split(dataset[,3], SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
str(training_set)
library(e1071)
classifier = svm(formula = diagnosis ~ .,
data = training_set,
type = 'C-classification',
kernel = "radial", gamma = 1, cost = 1, skala = TRUE)
print(classifier)
training_set <- as.data.frame(training_set )
plot(classifier, training_set)
test_set  <- as.data.frame(test_set )
plot(classifier, test_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
library(vtreat)
# convert wdbc.pcst to a dataframe
wdbc.pcst.df <- as.data.frame(wdbc.pcst)
nRows <- nrow(wdbc.pcst.df)
splitPlan <- kWayCrossValidation(nRows, 5, NULL, NULL)
# examine the split plan
str(splitPlan)
splitPlan
# Run a 10-fold cross validation plan from splitPlan
k <- 5
nRows <- nrow(wdbc.pcst.df)
splitPlan <- kWayCrossValidation(nRows, k, NULL, NULL)
# Run a 10-fold cross validation plan from splitPlan
for ( i in 1:k ) {
split <- splitPlan[[i]]
model <- svm(formula = diagnosis ~ .,
data = wdbc.pcst.df[split$train,],
type = 'C-classification',
kernel = "radial", gamma = 1, cost = 1, skala = TRUE)
model.pred.cv <- predict(model, newdata = wdbc.pcst.df[split$app,])
cat("Fold Cross Validation =",i,"->")
print(model)
plot(model, wdbc.pcst.df[split$app,])
cat("The accuracy of SVM classifier is =",100*mean(model.pred.cv==wdbc.pcst.df$diagnosis[split$app]),"%")
confMat <- table(model.pred.cv, wdbc.pcst.df$diagnosis[split$app])
print(confMat)
}
# installing library ElemStatLearn
library(ElemStatLearn)
# Convert the features of the data: wdbc.data
wdbc.data <- as.matrix(wdbc.data)
# Set the row names of wdbc.data
row.names(wdbc.data) <- wdbc$id
# Create diagnosis vector
diagnosis <- as.numeric(wdbc$diagnosis)
df <- read_csv("wdbc.csv")
library(tidyverse)
library(caret)
library(ggcorrplot)
library(tidyverse)
library(caret)
library(ggcorrplot)
library(GGally)
library(randomForest)
library(e1071)
library(ROCR)
library(pROC)
library(RCurl)
df <- read_csv("wdbc.csv")
df
library(textclean)
library(devtools)
library(katadasaR)
library(tokenizers)
library(stopwords)
library(dplyr)
library(proxy)
library(tm)
library(e1071)
library(kernlab)
library(caret)
library(gensvm)
library(corrplot)
library(superml)
library(remotes)
library(RTextTools)
data_latih <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\data-latih2.csv')
data_uji <- read.csv('C:\\Users\\Mrdart0\\Documents\\websiteR\\MulticlassSVM\\svm\\data-uji.csv')
y_train <- data_latih$Pernyataan
y_test <- data_uji$Pernyataan
pre_latih <- data_latih$Kalimat %>%
as.character()
pre_latih <- strip(pre_latih)
stemming_latih <- function(x){
paste(lapply(x,katadasar),collapse = " ")
}
pre_latih <- lapply(tokenize_words(pre_latih),stemming_latih)
hasil_preprocessing_latih <- tokenize_words(pre_latih)
hasil_preprocessing_latih
pre_uji <- data_uji$Kalimat %>%
as.character()
pre_uji<- strip(pre_uji)
stemming_uji <- function(x){
paste(lapply(x,katadasar),collapse = " ")
}
pre_uji <- lapply(tokenize_words(pre_uji),stemming_uji)
hasil_preprocessing_uji <- tokenize_words(pre_uji)
hasil_preprocessing_uji
TFIDF_latih <- function(hasil_preprocessing_latih){
#tf
tf_latih <- Corpus(VectorSource(hasil_preprocessing_latih))
tf_latih <- TermDocumentMatrix(tf_latih) %>% as.matrix()
#idf
idf_latih <- log(ncol(tf_latih) /  (1 + rowSums(tf_latih != 0))) %>% diag()
tf_idf_latih <- crossprod(tf_latih,idf_latih)
colnames(tf_idf_latih) <- rownames(tf_latih)
return(tf_idf_latih / sqrt(rowSums(tf_idf_latih^2)))
}
TFIDF_latih <- TFIDF_latih(hasil_preprocessing_latih)
TFIDF_latih
tf <- TfIdfVectorizer$new()
x_train_tf <- tf$fit_transform(hasil_preprocessing_latih)
x_test_tf <- tf$transform(hasil_preprocessing_uji)
x_test_tf
svm1 <- svm(factor(y_train)~.,data = x_train_tf,
method = "C-classification", kernal = "radial",
gamm = 0.1, cost = 10)
summary(svm1)
plot(factor(y_train),x_train_tf)
prediksi <- predict(svm1,x_test_tf)
pred2 <- predict(svm1,x_train_tf)
#table(pred)
xtab <- table(actual = factor(y_test), prediksi = pred)
